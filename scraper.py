#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v18 (SPA/JSON Decode Fix)
ç ´è§£åˆ©å˜‰é–£çš„ Single Page Application çµæ§‹ï¼Œå¾ JSON Script ä¸­æå–è³‡æ–™ã€‚
"""

import json
import os
import smtplib
import urllib.request
import re
import time
import random
import html  # [æ–°å¢] ç”¨ä¾†è§£ç¢¼ HTML
from datetime import datetime
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# --- Configuration ---
CONFIG = {
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
    "EMAIL_RECIPIENTS": ["acforgames9394@gmail.com"],
    "URL_28HSE": "https://www.28hse.com/buy/a3/dg45/c22902",
    "URL_RICA": "https://www.ricacorp.com/zh-hk/property/list/buy/%E5%A4%A9%E9%91%BD-estate-%E5%A4%A7%E5%9F%94%E5%8D%8A%E5%B1%B1-hma-hk"
}

def log(msg):
    print(msg, flush=True)

def get_headers():
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Version/17.3 Safari/605.1.15',
    ]
    return {
        'User-Agent': random.choice(agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Cookie': 'locale=zh-hk'
    }

def fetch_url(url):
    log(f"ğŸŒ Fetching: {url}")
    try:
        time.sleep(random.uniform(2, 4))
        req = urllib.request.Request(url, headers=get_headers())
        with urllib.request.urlopen(req, timeout=30) as resp:
            return resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        log(f"âŒ Fetch error ({url}): {e}")
        return None

# =============================================================================
# 1. 28Hse (ä¿æŒå®Œç¾ç‹€æ…‹)
# =============================================================================
def scrape_28hse():
    log("--- Scraping 28Hse ---")
    html_raw = fetch_url(CONFIG["URL_28HSE"])
    if not html_raw: return []
    if "Security Check" in html_raw: return []

    listings = []
    chunks = re.split(r'class="item', html_raw)
    
    for chunk in chunks[1:]:
        try:
            tower = 0; floor = "??"; unit = "?"
            full_desc_match = re.search(r'unit_desc"[^>]*>\s*(.*?)\s*<', chunk)
            if full_desc_match:
                full_text = full_desc_match.group(1)
                t_match = re.search(r'(\d+)\s*åº§', full_text)
                if t_match: tower = int(t_match.group(1))
                f_match = re.search(r'(ä½|ä¸­|é«˜)å±¤', full_text)
                if f_match: floor = f_match.group(1)
                u_match = re.search(r'([A-H])å®¤', full_text, re.IGNORECASE)
                if u_match: unit = u_match.group(1).upper()
            
            if tower == 0:
                t_match_backup = re.search(r'(?:ç¬¬|Block)?\s*(\d+)\s*åº§', chunk)
                if t_match_backup: tower = int(t_match_backup.group(1))

            if tower == 0: continue

            price_match = re.search(r'(?:\$|å”®)\s*([\d,]+)\s*è¬', chunk)
            if not price_match: continue
            price = int(price_match.group(1).replace(',', '')) * 10000

            clean_text = re.sub(r'<[^>]+>', ' ', chunk)
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            desc = clean_text[10:60] + "..."
            
            link_match = re.search(r'href="([^"]+)"', chunk)
            link = link_match.group(1) if link_match else CONFIG["URL_28HSE"]

            listing = {
                "id": f"28hse-{tower}-{price}",
                "tower": tower, "floor": floor, "unit": unit,
                "price": price, "raw_desc": desc, "url": link,
                "source": "hse28", "sourceName": "28Hse",
                "scrapedAt": datetime.now().isoformat()
            }
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   âœ… 28Hse: T{tower} {floor} {unit} ${price/10000}è¬")
        except: continue
    return listings

# =============================================================================
# 2. Ricacorp (é‡å° SPA JSON çµæ§‹)
# =============================================================================
def scrape_ricacorp():
    log("--- Scraping Ricacorp ---")
    raw_response = fetch_url(CONFIG["URL_RICA"])
    if not raw_response: return []
    
    # [é—œéµæ­¥é©Ÿ] åˆ©å˜‰é–£è³‡æ–™è—åœ¨ JSON Script å…§ï¼Œè€Œä¸”è¢«ç·¨ç¢¼äº† (e.g. &quot;)
    # å¿…é ˆå…ˆè§£ç¢¼ï¼ŒRegex æ‰èƒ½ç”Ÿæ•ˆ
    clean_html = html.unescape(raw_response)
    
    # æª¢æŸ¥æ˜¯å¦è¢« Incapsula é˜²ç«ç‰†æ“‹ä½
    if "Incapsula" in clean_html or "Request unsuccessful" in clean_html:
        log("ğŸš¨ Ricacorp Blocked by Firewall.")
        return []

    listings = []
    
    # ç­–ç•¥ï¼šå› ç‚ºè§£ç¢¼å¾Œè³‡æ–™æœƒè®Šæˆä¸€é•·ä¸²æ–‡å­—
    # æˆ‘å€‘æ ¹æ“šä½ æˆªåœ–çš„æ ¼å¼ "å¤©é‘½ 8åº§ 1æˆ¿" ä¾†æŠ“
    # å°‹æ‰¾: å¤©é‘½... (ä»»æ„ç©ºç™½) ... æ•¸å­— + åº§ ...... (400å­—å…§) ..... $ + æ•¸å­—
    
    # Regex è§£é‡‹:
    # å¤©é‘½\s+            -> å¿…é ˆè¦‹åˆ°å¤©é‘½
    # (?:ç¬¬)?(\d+)\s*åº§  -> æŠ“åº§è™Ÿ (Group 1)
    # .{0,400}?         -> å¾€å¾Œæ‰¾
    # \$([\d,]+)        -> æŠ“åƒ¹éŒ¢ (Group 2)
    
    pattern = r'å¤©é‘½\s+(?:ç¬¬)?(\d+)\s*åº§.{0,400}?\$\s*([\d,]+)'
    
    matches = re.finditer(pattern, clean_html)
    found_count = 0
    
    for match in matches:
        try:
            tower = int(match.group(1))
            price = int(match.group(2).replace(',', '')) * 10000
            
            # å˜—è©¦åœ¨åŒ¹é…åˆ°çš„æ–‡å­—å‘¨åœæ‰¾æ¨“å±¤ (ä½/ä¸­/é«˜)
            full_match_text = match.group(0)
            floor = "??"
            f_match = re.search(r'(ä½|ä¸­|é«˜)å±¤', full_match_text)
            if f_match: floor = f_match.group(1)
            
            # åˆ©å˜‰é–£é€£çµ (å›ºå®šå‰ç¶´ + æœå°‹åƒæ•¸ï¼Œå› ç‚ºé›£ä»¥å¾ JSON æŠ“æº–ç¢ºé€£çµ)
            link = CONFIG["URL_RICA"]
            desc = f"ç¬¬{tower}åº§ (åˆ©å˜‰é–£)"

            listing = {
                "id": f"rica-{tower}-{price}",
                "tower": tower, "floor": floor, "unit": "?",
                "price": price, "raw_desc": desc, "url": link,
                "source": "centanet", # æ©™è‰²æ¨™ç±¤
                "sourceName": "Ricacorp",
                "scrapedAt": datetime.now().isoformat()
            }
            
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                found_count += 1
                log(f"   âœ… Ricacorp: T{tower} ${price/10000}è¬")
        except: continue
    
    if found_count == 0:
        log("âš ï¸ Ricacorp connected but 0 listings found. Structure might differ.")
        
    return listings

# --- Main ---
def main():
    log("ğŸš€ Starting Scraper v18 (JSON Fix)...")
    
    seen_ids = set()
    try:
        if Path(CONFIG["CACHE_FILE"]).exists():
            data = json.loads(Path(CONFIG["CACHE_FILE"]).read_text())
            seen_ids = set(data.get("seen_ids", []))
    except: pass

    all_listings = []
    all_listings.extend(scrape_28hse())
    all_listings.extend(scrape_ricacorp())
    
    log(f"ğŸ“Š Total Found: {len(all_listings)}")
    
    all_listings.sort(key=lambda x: (x['tower'], x['price']))

    # Email
    new_listings = [l for l in all_listings if l["id"] not in seen_ids]
    sender = os.environ.get("EMAIL_SENDER")
    password = os.environ.get("EMAIL_PASSWORD")
    
    if new_listings and sender and password:
        subject = f"ğŸ”¥ å¤©é‘½æ–°ç›¤é€šå ± ({len(new_listings)})"
        lines = ["æœ€æ–°æ”¾ç›¤ (28Hse + åˆ©å˜‰é–£):", ""]
        for l in new_listings:
            loc = f"{l['floor']}å±¤ {l['unit']}å®¤" if l['unit'] != "?" else ""
            lines.append(f"ğŸ“ ç¬¬ {l['tower']} åº§ {loc} | ${l['price']/10000:,.0f}è¬ | {l['sourceName']}")
            lines.append(f"   {l['url']}")
            lines.append("")
        lines.append("Dashboard: https://spokenelam.github.io/sky-diamond-tracker/")
        
        msg = MIMEMultipart()
        msg['From'] = sender
        msg['To'] = ", ".join(CONFIG["EMAIL_RECIPIENTS"])
        msg['Subject'] = subject
        msg.attach(MIMEText("\n".join(lines), 'plain', 'utf-8'))
        
        try:
            with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
                server.login(sender, password)
                server.sendmail(sender, CONFIG["EMAIL_RECIPIENTS"], msg.as_string())
            log("ğŸ“§ Email sent.")
        except: pass

    # Save
    current_ids = list(seen_ids)
    for l in all_listings:
        if l["id"] not in current_ids:
            current_ids.append(l["id"])
            
    Path("data").mkdir(exist_ok=True)
    output_data = {"lastUpdate": datetime.now().isoformat(), "listings": all_listings}
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    
    cache_data = {"last_run": datetime.now().isoformat(), "seen_ids": current_ids}
    Path(CONFIG["CACHE_FILE"]).write_text(json.dumps(cache_data, indent=2))
    log("ğŸ’¾ Data saved.")

if __name__ == "__main__":
    main()
