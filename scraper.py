#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v16 (28Hse + Ricacorp Robust Mode)
é‡å°åˆ©å˜‰é–£åŠ å…¥æš´åŠ›æœå°‹é‚è¼¯ï¼Œç¢ºä¿æŠ“å–æˆåŠŸç‡
"""

import json
import os
import smtplib
import urllib.request
import re
import time
import random
from datetime import datetime
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# --- Configuration ---
CONFIG = {
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
    "EMAIL_RECIPIENTS": ["acforgames9394@gmail.com"],
    # ç¶²å€
    "URL_28HSE": "https://www.28hse.com/buy/a3/dg45/c22902",
    "URL_RICA": "https://www.ricacorp.com/zh-hk/property/list/buy/%E5%A4%A9%E9%91%BD-estate-%E5%A4%A7%E5%9F%94%E5%8D%8A%E5%B1%B1-hma-hk"
}

def log(msg):
    print(msg, flush=True)

def get_headers():
    # éš¨æ©Ÿ User-Agent
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Version/17.3 Safari/605.1.15',
    ]
    return {
        'User-Agent': random.choice(agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-HK,zh;q=0.9,en;q=0.8',
    }

def fetch_url(url):
    log(f"ğŸŒ Fetching: {url}")
    try:
        time.sleep(random.uniform(2, 5)) # éš¨æ©Ÿä¼‘æ¯ï¼Œé¿é–‹å°é–
        req = urllib.request.Request(url, headers=get_headers())
        with urllib.request.urlopen(req, timeout=30) as resp:
            return resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        log(f"âŒ Fetch error ({url}): {e}")
        return None

# =============================================================================
# 1. 28Hse Scraper (ä¿æŒä½ å‰›æ‰æˆåŠŸçš„é‚è¼¯)
# =============================================================================
def scrape_28hse():
    log("--- Scraping 28Hse ---")
    html = fetch_url(CONFIG["URL_28HSE"])
    if not html: return []
    if "Security Check" in html: 
        log("ğŸš¨ 28Hse Blocked.")
        return []

    listings = []
    chunks = re.split(r'class="item', html)
    
    for chunk in chunks[1:]:
        try:
            tower = 0; floor = "??"; unit = "?"
            
            # å„ªå…ˆ: unit_desc
            full_desc_match = re.search(r'unit_desc"[^>]*>\s*(.*?)\s*<', chunk)
            if full_desc_match:
                full_text = full_desc_match.group(1)
                t_match = re.search(r'(\d+)\s*åº§', full_text)
                if t_match: tower = int(t_match.group(1))
                f_match = re.search(r'(ä½|ä¸­|é«˜)å±¤', full_text)
                if f_match: floor = f_match.group(1)
                u_match = re.search(r'([A-H])å®¤', full_text, re.IGNORECASE)
                if u_match: unit = u_match.group(1).upper()
            
            # å¾Œå‚™: é€šç”¨
            if tower == 0:
                t_match_backup = re.search(r'(?:ç¬¬|Block)?\s*(\d+)\s*åº§', chunk)
                if t_match_backup: tower = int(t_match_backup.group(1))

            if tower == 0: continue

            price_match = re.search(r'(?:\$|å”®)\s*([\d,]+)\s*è¬', chunk)
            if not price_match: continue
            price = int(price_match.group(1).replace(',', '')) * 10000

            clean_text = re.sub(r'<[^>]+>', ' ', chunk)
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            desc = clean_text[10:60] + "..."
            
            link_match = re.search(r'href="([^"]+)"', chunk)
            link = link_match.group(1) if link_match else CONFIG["URL_28HSE"]

            listing = {
                "id": f"28hse-{tower}-{price}",
                "tower": tower, "floor": floor, "unit": unit,
                "price": price, "raw_desc": desc, "url": link,
                "source": "hse28", "sourceName": "28Hse",
                "scrapedAt": datetime.now().isoformat()
            }
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   âœ… 28Hse: T{tower} {floor} {unit} ${price/10000}è¬")
        except: continue
    return listings

# =============================================================================
# 2. Ricacorp Scraper (æ”¹ç”¨æš´åŠ›æœå°‹)
# =============================================================================
def scrape_ricacorp():
    log("--- Scraping Ricacorp ---")
    html = fetch_url(CONFIG["URL_RICA"])
    if not html: return []
    
    # æª¢æŸ¥æ˜¯å¦è¢«æ“‹ (Debug)
    title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    if title_match: log(f"   Page Title: {title_match.group(1)}")
    if "Security" in html or "Incapsula" in html:
        log("ğŸš¨ Ricacorp Blocked (Incapsula/WAF).")
        return []

    listings = []
    
    # ç­–ç•¥: å…¨æ–‡æœç´¢
    # å°‹æ‰¾: (ç¬¬) + æ•¸å­— + åº§ ......(300å­—å…§)...... $ + æ•¸å­— + è¬
    # Regex: (?:ç¬¬)?\s*(\d+)\s*åº§.{0,300}?\$\s*([\d,]+)\s*è¬
    
    clean_html = html.replace('\n', ' ').replace('\r', '')
    pattern = r'(?:ç¬¬)?\s*(\d+)\s*åº§.{0,300}?\$\s*([\d,]+)\s*è¬'
    
    matches = re.finditer(pattern, clean_html)
    
    for match in matches:
        try:
            tower = int(match.group(1))
            price = int(match.group(2).replace(',', '')) * 10000
            
            # å˜—è©¦å¾ match åˆ°æ—¢æ–‡å­—è£¡é¢æµå¤šDè³‡æ–™
            raw_text = match.group(0)
            
            # æ‰¾æ¨“å±¤
            floor = "??"
            f_match = re.search(r'(ä½|ä¸­|é«˜)å±¤', raw_text)
            if f_match: floor = f_match.group(1)
            
            # æ‰¾é€£çµ (å¾€å›æ‰¾ href)
            # é€™è£¡æ¯”è¼ƒé›£æº–ç¢ºæŠ“å°æ‡‰ linkï¼Œæˆ‘å€‘å…ˆç”¨åˆ—è¡¨é  link
            link = CONFIG["URL_RICA"]
            
            desc = f"ç¬¬{tower}åº§ (åˆ©å˜‰é–£ä¾†æº)"

            listing = {
                "id": f"rica-{tower}-{price}",
                "tower": tower, "floor": floor, "unit": "?",
                "price": price, "raw_desc": desc, "url": link,
                "source": "centanet", # å€Ÿç”¨æ©™è‰²æ¨£å¼
                "sourceName": "Ricacorp",
                "scrapedAt": datetime.now().isoformat()
            }
            
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   âœ… Ricacorp: T{tower} ${price/10000}è¬")
                
        except: continue
        
    return listings

# --- Main ---
def main():
    log("ğŸš€ Starting Scraper v16 (Multi-Source)...")
    
    # Cache
    seen_ids = set()
    try:
        if Path(CONFIG["CACHE_FILE"]).exists():
            data = json.loads(Path(CONFIG["CACHE_FILE"]).read_text())
            seen_ids = set(data.get("seen_ids", []))
    except: pass

    all_listings = []
    
    # åŸ·è¡Œå…©å€‹ä¾†æº
    all_listings.extend(scrape_28hse())
    all_listings.extend(scrape_ricacorp())
    
    log(f"ğŸ“Š Total Found: {len(all_listings)}")
    
    # Sort
    all_listings.sort(key=lambda x: (x['tower'], x['price']))

    # Email
    new_listings = [l for l in all_listings if l["id"] not in seen_ids]
    sender = os.environ.get("EMAIL_SENDER")
    password = os.environ.get("EMAIL_PASSWORD")
    
    if new_listings and sender and password:
        subject = f"ğŸ”¥ å¤©é‘½æ–°ç›¤é€šå ± ({len(new_listings)})"
        lines = ["æœ€æ–°æ”¾ç›¤ (28Hse + åˆ©å˜‰é–£):", ""]
        for l in new_listings:
            loc = f"{l['floor']}å±¤ {l['unit']}å®¤" if l['unit'] != "?" else ""
            lines.append(f"ğŸ“ ç¬¬ {l['tower']} åº§ {loc} | ${l['price']/10000:,.0f}è¬ | {l['sourceName']}")
            lines.append(f"   {l['url']}")
            lines.append("")
        lines.append("Dashboard: https://spokenelam.github.io/sky-diamond-tracker/")
        
        msg = MIMEMultipart()
        msg['From'] = sender
        msg['To'] = ", ".join(CONFIG["EMAIL_RECIPIENTS"])
        msg['Subject'] = subject
        msg.attach(MIMEText("\n".join(lines), 'plain', 'utf-8'))
        
        try:
            with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
                server.login(sender, password)
                server.sendmail(sender, CONFIG["EMAIL_RECIPIENTS"], msg.as_string())
            log("ğŸ“§ Email sent.")
        except: pass

    # Save
    current_ids = list(seen_ids)
    for l in all_listings:
        if l["id"] not in current_ids:
            current_ids.append(l["id"])
            
    Path("data").mkdir(exist_ok=True)
    output_data = {"lastUpdate": datetime.now().isoformat(), "listings": all_listings}
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    
    cache_data = {"last_run": datetime.now().isoformat(), "seen_ids": current_ids}
    Path(CONFIG["CACHE_FILE"]).write_text(json.dumps(cache_data, indent=2))
    log("ğŸ’¾ Data saved.")

if __name__ == "__main__":
    main()
