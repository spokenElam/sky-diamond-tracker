#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v8 (Search Query + Proximity Regex)
"""

import json
import urllib.request
import urllib.parse
import re
import random
import time
from datetime import datetime
from pathlib import Path

# --- Configuration ---
CONFIG = {
    "TARGET_TOWERS": [8, 9, 10, 11, 12, 13, 15, 16, 18],
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
}

def log(msg):
    print(msg, flush=True)

def get_headers():
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15',
    ]
    return {
        'User-Agent': random.choice(agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Cookie': 'locale=zh-hk', # å¼·åˆ¶ä¸­æ–‡
    }

def fetch_url(url):
    log(f"ğŸŒ Fetching: {url}")
    try:
        time.sleep(random.uniform(2, 4)) # ä¼‘æ¯ä¹…ä¸€é»
        req = urllib.request.Request(url, headers=get_headers())
        with urllib.request.urlopen(req, timeout=30) as resp:
            return resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        log(f"âŒ Error fetching {url}: {e}")
        return None

# =============================================================================
# 1. 28Hse (Search Query Mode)
# =============================================================================
def scrape_28hse():
    log("--- Scraping 28Hse (Search Mode) ---")
    
    # ä½¿ç”¨æœå°‹ URL (å¤©é‘½ encoded)
    # q=å¤©é‘½
    search_url = "https://www.28hse.com/buy?q=%E5%A4%A9%E9%91%BD"
    
    html = fetch_url(search_url)
    if not html: return []

    # Check Title
    title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    log(f"   Page Title: {title_match.group(1) if title_match else 'Unknown'}")

    listings = []
    
    # ç­–ç•¥ï¼šä¸å†åˆ‡å‰²å€å¡Šï¼Œç›´æ¥æ‰¾æ‰€æœ‰ "åº§" çš„ä½ç½®
    # ç„¶å¾Œå¾€å¾Œæ‰¾ 200 å€‹å­—å…ƒå…§çš„ "åƒ¹éŒ¢"
    
    # 1. æ‰¾å‡ºæ‰€æœ‰ "Xåº§" çš„ä½ç½®
    # pattern: æ•¸å­— + åº§ (å¿½ç•¥å‰é¢çš„ ç¬¬/Block)
    # ä½¿ç”¨ finditer ç²å–ä½ç½®
    tower_iter = re.finditer(r'(?:ç¬¬|Block|T)?\s*(\d+)\s*åº§', html)
    
    for match in tower_iter:
        try:
            tower = int(match.group(1))
            start_pos = match.end()
            
            # åªè™•ç†ç›®æ¨™åº§æ•¸
            if tower not in CONFIG["TARGET_TOWERS"]:
                continue
                
            # åœ¨é€™å€‹ "åº§" ä¹‹å¾Œçš„ 300 å€‹å­—å…ƒå…§æ‰¾åƒ¹éŒ¢
            # é€™æ˜¯ "Proximity Search" (é„°è¿‘æœå°‹)
            search_window = html[start_pos : start_pos + 300]
            
            # æ‰¾åƒ¹éŒ¢ ($xxxè¬ or å”®xxxè¬)
            price_match = re.search(r'(?:\$|å”®)\s*([\d,]+)\s*è¬', search_window)
            
            if price_match:
                price = int(price_match.group(1).replace(',', '')) * 10000
                
                # å»ºç«‹ ID
                listing_id = f"28hse-{tower}-{price}"
                
                # å˜—è©¦åœ¨ window å…§æ‰¾æè¿°
                clean_text = re.sub(r'<[^>]+>', ' ', search_window)
                desc = f"ç¬¬{tower}åº§ " + clean_text[:30].strip() + "..."

                # å˜—è©¦æ‰¾ Link (é€šå¸¸åœ¨å‰é¢)
                # å¾€å›æ‰¾ href
                link = search_url # Default
                
                listing = {
                    "id": listing_id,
                    "tower": tower,
                    "floor": "??", "unit": "?", "size": 0, "rooms": 0,
                    "price": price, "pricePerFt": 0,
                    "raw_desc": desc,
                    "url": link,
                    "source": "hse28",
                    "sourceName": "28Hse",
                    "scrapedAt": datetime.now().isoformat()
                }
                
                if not any(l["id"] == listing["id"] for l in listings):
                    listings.append(listing)
                    log(f"   âœ… Found: T{tower} ${price/10000}è¬")
        except:
            continue
            
    return listings

# =============================================================================
# 2. Property.hk (Robust Regex)
# =============================================================================
def scrape_property_hk():
    log("--- Scraping Property.hk ---")
    url = "https://www.property.hk/buy/search/%E5%A4%A9%E9%91%BD/"
    html = fetch_url(url)
    if not html: return []
    
    listings = []
    
    # åŒæ¨£ä½¿ç”¨é„°è¿‘æœå°‹æ³•
    tower_iter = re.finditer(r'ç¬¬\s*(\d+)\s*åº§', html)
    
    for match in tower_iter:
        try:
            tower = int(match.group(1))
            if tower not in CONFIG["TARGET_TOWERS"]: continue
            
            start_pos = match.end()
            search_window = html[start_pos : start_pos + 300]
            
            price_match = re.search(r'(\d{3,5})\s*è¬', search_window)
            
            if price_match:
                price = int(price_match.group(1)) * 10000
                
                listing = {
                    "id": f"phk-{tower}-{price}",
                    "tower": tower,
                    "floor": "??", "unit": "?", "size": 0, "rooms": 0,
                    "price": price, "pricePerFt": 0,
                    "raw_desc": f"ç¬¬{tower}åº§ (Property.hk)",
                    "url": url,
                    "source": "hkp",
                    "sourceName": "Property.hk",
                    "scrapedAt": datetime.now().isoformat()
                }
                
                if not any(l["id"] == listing["id"] for l in listings):
                    listings.append(listing)
                    log(f"   âœ… Found: T{tower} ${price/10000}è¬")
        except: continue
            
    return listings

# =============================================================================
# MAIN
# =============================================================================
def main():
    log("ğŸš€ Starting Scraper v8 (Search Query Mode)...")
    all_listings = []
    
    all_listings.extend(scrape_28hse())
    all_listings.extend(scrape_property_hk())
    
    log(f"ğŸ“Š Total Combined Listings: {len(all_listings)}")
    
    all_listings.sort(key=lambda x: x['price'])

    Path("data").mkdir(exist_ok=True)
    output_data = {
        "lastUpdate": datetime.now().isoformat(),
        "listings": all_listings
    }
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    log("ğŸ’¾ Data saved.")

if __name__ == "__main__":
    main()
