#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v10 (Direct Estate ID + No PropertyHK)
"""

import json
import urllib.request
import re
from datetime import datetime
from pathlib import Path
import time
import random

# --- Configuration ---
CONFIG = {
    "TARGET_TOWERS": [8, 9, 10, 11, 12, 13, 15, 16, 18],
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
    # [ä¿®æ­£] ä½¿ç”¨å¤©é‘½å°ˆå±¬ ID (16716) ç›´é€£é é¢ï¼Œç¢ºä¿ä¸€å®šä¿‚å¤©é‘½
    "URL": "https://www.28hse.com/buy/residential/property/16716"
}

def log(msg):
    print(msg, flush=True)

def fetch_url(url):
    log(f"ğŸŒ Fetching: {url}")
    # æ¨¡æ“¬ç°¡å–®ç€è¦½å™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-HK,zh;q=0.9,en;q=0.8',
        'Referer': 'https://www.28hse.com/',
        'Cookie': 'locale=zh-hk'
    }
    try:
        # éš¨æ©Ÿç­‰å¾… 2 ç§’ï¼Œæ‰®çœŸäºº
        time.sleep(2)
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as resp:
            return resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        log(f"âŒ Fetch error: {e}")
        return None

def scrape_28hse():
    log("--- Scraping 28Hse (Direct Estate ID Mode) ---")
    
    html = fetch_url(CONFIG["URL"])
    if not html: return []

    # Debug: å°å‡ºæ¨™é¡Œç¢ºèªä¿‚å’ªå¤©é‘½é é¢
    title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    if title_match:
        log(f"   Page Title: {title_match.group(1)}")

    listings = []
    
    # ç­–ç•¥ï¼šæš´åŠ›æœå°‹æ•´é æ–‡å­—
    # å°‹æ‰¾ï¼š (ç¬¬/Block/T)? + æ•¸å­— + åº§ ......(ä¸­é–“éš¨ä¾¿éš” 300å­—)...... $ + æ•¸å­— + è¬
    
    # regex: 
    # (?:ç¬¬|Block|T)?  -> å‰é¢å¯èƒ½æœ‰ "ç¬¬" å­—ï¼Œä¹Ÿå¯èƒ½ç„¡ (ä¾‹å¦‚ "13åº§")
    # \s*(\d+)\s*åº§    -> æ‰¾åº§è™Ÿ
    
    pattern = r'(?:ç¬¬|Block|T)?\s*(\d+)\s*åº§.{0,300}?\$\s*([\d,]+)\s*è¬'
    
    # ç§»é™¤æ›è¡Œç¬¦è™Ÿï¼Œè®Šæˆä¸€è¡Œéæ–¹ä¾¿æœå°‹
    clean_html = html.replace('\n', ' ').replace('\r', '')
    
    matches = re.finditer(pattern, clean_html)
    
    for match in matches:
        try:
            tower = int(match.group(1))
            price_str = match.group(2).replace(',', '')
            price = int(price_str) * 10000
            
            # ç¯©é¸åº§æ•¸
            if tower not in CONFIG["TARGET_TOWERS"]:
                continue

            # å»ºç«‹ ID
            listing_id = f"28hse-{tower}-{price}"
            
            # æŠ“å–æè¿° (å‰å¾Œæ–‡)
            raw_text = match.group(0)
            # ç§»é™¤ HTML tag
            clean_desc = re.sub(r'<[^>]+>', ' ', raw_text)
            clean_desc = re.sub(r'\s+', ' ', clean_desc).strip()
            desc = clean_desc[:60] + "..."

            listing = {
                "id": listing_id,
                "tower": tower,
                "floor": "??", 
                "unit": "?", 
                "size": 0, 
                "rooms": 0,
                "price": price, 
                "pricePerFt": 0,
                "raw_desc": desc,
                "url": CONFIG["URL"],
                "source": "hse28",
                "sourceName": "28Hse",
                "scrapedAt": datetime.now().isoformat()
            }
            
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   âœ… Found: T{tower} ${price/10000}è¬")
                
        except Exception as e:
            continue
            
    return listings

# =============================================================================
# MAIN
# =============================================================================
def main():
    log("ğŸš€ Starting Scraper v10...")
    
    all_listings = scrape_28hse()
    
    log(f"ğŸ“Š Total Listings: {len(all_listings)}")
    
    # Sort
    all_listings.sort(key=lambda x: x['price'])

    # Save
    Path("data").mkdir(exist_ok=True)
    output_data = {
        "lastUpdate": datetime.now().isoformat(),
        "listings": all_listings
    }
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    log("ğŸ’¾ Data saved.")

if __name__ == "__main__":
    main()
