#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v4 (Fix URL & Loose Regex)
"""

import json
import os
import sys
import traceback
import urllib.request
import re
import random
import time
from datetime import datetime
from pathlib import Path

# --- Configuration ---
CONFIG = {
    "TARGET_TOWERS": [8, 9, 10, 11, 12, 13, 15, 16, 18],
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
    # [ä¿®æ­£] æ”¹å›åŸæœ¬æœ‰æ•ˆçš„æœå°‹çµæœé€£çµ (å¤©é‘½å°ˆé )
    "URL": "https://www.28hse.com/utf8/buy/residential/a3/dg45/c22902"
}

# --- Helpers ---
def log(msg):
    print(msg, flush=True)

def get_random_user_agent():
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0'
    ]
    return random.choice(agents)

# --- Scraper Logic ---
def fetch_html(url):
    log(f"ğŸŒ Fetching URL: {url}")
    
    headers = {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-HK,zh;q=0.9,en;q=0.8',
        'Referer': 'https://www.28hse.com/',
        'Cookie': 'locale=zh-hk'
    }

    try:
        req = urllib.request.Request(url, headers=headers)
        time.sleep(2) 
        with urllib.request.urlopen(req, timeout=30) as resp:
            html = resp.read().decode('utf-8', errors='ignore')
            return html
    except Exception as e:
        log(f"âŒ Network Error: {e}")
        # å¦‚æœæ˜¯ 404ï¼Œå°å‡ºæ›´å¤šè³‡è¨Š
        if hasattr(e, 'code'):
            log(f"   Status Code: {e.code}")
        return None

def parse_listings(html):
    listings = []
    
    # 1. æª¢æŸ¥æ¨™é¡Œ
    title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    page_title = title_match.group(1) if title_match else "No Title Found"
    log(f"ğŸ“„ Page Title: [{page_title}]")
    
    if "Security" in page_title or "Just a moment" in page_title:
        log("âš ï¸ WARNING: GitHub IP blocked by Cloudflare.")
        return []

    # 2. å¯¬é¬†æŠ“å– (Pattern: ç¬¬Xåº§ ... åƒ¹éŒ¢)
    log("ğŸ” Extracting data...")
    
    # æ¸…ç† HTML æ¨™ç±¤ï¼Œè®Šæˆç´”æ–‡å­—
    clean_text = re.sub(r'<[^>]+>', ' ', html)
    clean_text = re.sub(r'\s+', ' ', clean_text) # æŠŠå¤šé¤˜ç©ºç™½è®Šæˆå–®ä¸€ç©ºç™½
    
    # æŠ“å–é‚è¼¯ï¼šæ‰¾ "ç¬¬Xåº§" æˆ– "Xåº§"ï¼Œå¾Œé¢è·Ÿè‘— "æ•¸å­—+è¬"
    # ä¾‹å¦‚: "ç¬¬ 8 åº§ ... $ 638 è¬"
    # Group 1: åº§æ•¸
    # Group 2: åƒ¹éŒ¢
    pattern = r'ç¬¬?\s*(\d+)\s*åº§.{0,150}?\$?([\d,]+(?:\.\d+)?)\s*è¬'
    
    matches = re.findall(pattern, clean_text)
    log(f"   Found {len(matches)} potential matches")

    for match in matches:
        try:
            tower_str = match[0]
            price_str = match[1].replace(',', '').replace(' ', '')
            
            tower = int(tower_str)
            price = int(float(price_str) * 10000)
            
            # éæ¿¾
            if tower not in CONFIG["TARGET_TOWERS"]:
                continue
                
            # å»ºç«‹è³‡æ–™
            # å› ç‚ºæ˜¯å¾ç´”æ–‡å­—æŠ“ï¼Œæ²’æœ‰æº–ç¢ºçš„ URL å°æ‡‰åˆ°è©²ç›¤ï¼Œæ‰€ä»¥ç”¨ä¸»æœå°‹é é¢ URL
            listing_id = f"{tower}-{price}" 
            
            listing = {
                "id": listing_id,
                "tower": tower,
                "floor": "??", 
                "unit": "?",
                "size": 0,    # æš«æ™‚è¨­ç‚º 0ï¼Œé¿å… regex éŒ¯èª¤
                "rooms": 0,
                "price": price,
                "pricePerFt": 0,
                "raw_desc": f"ç¬¬{tower}åº§ (HK${price_str}è¬)",
                "url": CONFIG["URL"],
                "source": "28hse",
                "sourceName": "28Hse",
                "scrapedAt": datetime.now().isoformat()
            }
            
            # å»é™¤é‡è¤‡
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   âœ… Matched: Tower {tower} @ ${price_str}è¬")

        except Exception as e:
            # log(f"   Parse Error: {e}")
            continue

    return listings

# --- Main ---
def main():
    log("ğŸš€ Starting Scraper v4...")
    
    # Fetch
    html = fetch_html(CONFIG["URL"])
    current_listings = []
    
    if html:
        current_listings = parse_listings(html)
    else:
        log("âŒ No HTML content retrieved.")

    log(f"ğŸ“Š Total Listings Found: {len(current_listings)}")

    # Update Data
    Path("data").mkdir(exist_ok=True)
    
    output_data = {
        "lastUpdate": datetime.now().isoformat(),
        "listings": current_listings
    }
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    
    # é€™è£¡æˆ‘å€‘ä¸å¯« cacheï¼Œå› ç‚ºæ¯æ¬¡éƒ½é‡æ–°æŠ“å–æœ€æ–°çš„ç‹€æ…‹
    # ä½ çš„ tracker.py åŸæœ¬æœ‰ç”¨ cacheï¼Œä½†å¦‚æœæ˜¯ v4 ç´”é¡¯ç¤ºæ¨¡å¼ï¼Œå¯ä»¥ç°¡åŒ–
    
    log("ğŸ’¾ Data saved.")

if __name__ == "__main__":
    main()
