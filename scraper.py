#!/usr/bin/env python3
"""
Â§©ÈëΩ The Regent - Scraper v21 (28Hse + Squarefoot + Centaline)
‰∏âÂè∞ËÅØÊí≠ÔºöÂòóË©¶Âä†ÂÖ•‰∏≠Âéü (Centaline) ÊäìÂèñÈÇèËºØ„ÄÇ
"""

import json
import os
import smtplib
import urllib.request
import re
import time
import random
from datetime import datetime
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# --- Configuration ---
CONFIG = {
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
    "EMAIL_RECIPIENTS": ["acforgames9394@gmail.com"],
    # Á∂≤ÂùÄË®≠ÂÆö
    "URL_28HSE": "https://www.28hse.com/buy/a3/dg45/c22902",
    "URL_SQFT": "https://www.squarefoot.com.hk/buy?propertyDoSearchVersion=2.0&searchText=%E5%A4%A9%E9%91%BD&locations=&district_group_hk=&district_group_kw=&district_group_nt=&district_group_islands=&district_group_sch_pri=&district_group_sch_sec=&district_group_university=&price=&price=&mainType=&roomRange=",
    "URL_CENTA": "https://hk.centanet.com/findproperty/list/buy/-%E5%A4%A9%E9%91%BD_2-DEPPWPPJPB"
}

def log(msg):
    print(msg, flush=True)

def get_headers():
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Version/17.3 Safari/605.1.15',
    ]
    return {
        'User-Agent': random.choice(agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Cookie': 'locale=zh-hk'
    }

def fetch_url(url):
    log(f"üåç Fetching: {url}")
    try:
        time.sleep(random.uniform(3, 6)) # ‰∏≠ÂéüÈúÄË¶Å‰ºëÊÅØÊõ¥‰πÖ
        req = urllib.request.Request(url, headers=get_headers())
        with urllib.request.urlopen(req, timeout=30) as resp:
            return resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        log(f"‚ùå Fetch error ({url}): {e}")
        return None

# =============================================================================
# 1. 28Hse (v14 Cleaner)
# =============================================================================
def scrape_28hse():
    log("--- Scraping 28Hse ---")
    html_raw = fetch_url(CONFIG["URL_28HSE"])
    if not html_raw: return []
    if "Security Check" in html_raw: return []

    listings = []
    chunks = re.split(r'class="item', html_raw)
    
    for chunk in chunks[1:]:
        try:
            tower = 0; floor = "??"; unit = "?"
            full_desc_match = re.search(r'unit_desc"[^>]*>\s*(.*?)\s*<', chunk)
            if full_desc_match:
                full_text = full_desc_match.group(1)
                t_match = re.search(r'(\d+)\s*Â∫ß', full_text)
                if t_match: tower = int(t_match.group(1))
                f_match = re.search(r'(‰Ωé|‰∏≠|È´ò)Â±§', full_text)
                if f_match: floor = f_match.group(1)
                u_match = re.search(r'([A-H])ÂÆ§', full_text, re.IGNORECASE)
                if u_match: unit = u_match.group(1).upper()
            
            if tower == 0:
                t_match_backup = re.search(r'(?:Á¨¨|Block)?\s*(\d+)\s*Â∫ß', chunk)
                if t_match_backup: tower = int(t_match_backup.group(1))
            if tower == 0: continue

            price_match = re.search(r'(?:\$|ÂîÆ)\s*([\d,]+)\s*Ëê¨', chunk)
            if not price_match: continue
            price = int(price_match.group(1).replace(',', '')) * 10000

            clean_text = re.sub(r'<[^>]+>', ' ', chunk)
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            desc = clean_text[10:60] + "..."
            
            link_match = re.search(r'href="([^"]+)"', chunk)
            link = link_match.group(1) if link_match else CONFIG["URL_28HSE"]

            listing = {
                "id": f"28hse-{tower}-{price}",
                "tower": tower, "floor": floor, "unit": unit,
                "price": price, "raw_desc": desc, "url": link,
                "source": "hse28", "sourceName": "28Hse",
                "scrapedAt": datetime.now().isoformat()
            }
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   ‚úÖ 28Hse: T{tower} {floor} {unit} ${price/10000}Ëê¨")
        except: continue
    return listings

# =============================================================================
# 2. Squarefoot (v20)
# =============================================================================
def scrape_squarefoot():
    log("--- Scraping Squarefoot ---")
    html_raw = fetch_url(CONFIG["URL_SQFT"])
    if not html_raw: return []
    if "Security" in html_raw: return []

    listings = []
    clean_html = html_raw.replace('\n', ' ')
    pattern = r'(\d+)\s*Â∫ß.{0,600}?ÂîÆ\s*\$([\d,]+)'
    matches = re.finditer(pattern, clean_html)
    
    for match in matches:
        try:
            tower = int(match.group(1))
            if tower < 1 or tower > 20: continue 
            price = int(match.group(2).replace(',', '')) * 10000
            
            raw_text = match.group(0)
            floor = "??"
            f_match = re.search(r'(‰Ωé|‰∏≠|È´ò)Â±§', raw_text)
            if f_match: floor = f_match.group(1)
            
            link = CONFIG["URL_SQFT"]
            desc = f"Á¨¨{tower}Â∫ß (Squarefoot)"

            listing = {
                "id": f"sqft-{tower}-{price}",
                "tower": tower, "floor": floor, "unit": "?",
                "price": price, "raw_desc": desc, "url": link,
                "source": "hkp", "sourceName": "Squarefoot", # Á¥´Ëâ≤
                "scrapedAt": datetime.now().isoformat()
            }
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   ‚úÖ Squarefoot: T{tower} ${price/10000}Ëê¨")
        except: continue
    return listings

# =============================================================================
# 3. Centaline (New - ÈáùÂ∞ç Nuxt ÁµêÊßã)
# =============================================================================
def scrape_centaline():
    log("--- Scraping Centaline ---")
    html_raw = fetch_url(CONFIG["URL_CENTA"])
    if not html_raw: return []
    
    # Ê™¢Êü•ÊòØÂê¶Ë¢´ Incapsula Â∞ÅÈéñ
    if "Incapsula" in html_raw or "Request unsuccessful" in html_raw:
        log("üö® Centaline Blocked (Incapsula).")
        return []

    listings = []
    clean_html = html_raw.replace('\n', ' ')
    
    # ‰∏≠ÂéüÂàóË°®È†ÅÈÄöÂ∏∏‰∏çÈ°ØÁ§∫Â∫ßÊï∏ÔºåÂè™È°ØÁ§∫ "ÂØ¶Áî® XXXÂëé ... $XXXËê¨"
    # Âõ†ÁÇ∫Á∂≤ÂùÄÂ∑≤Á∂ì Filter Â∑¶Â§©ÈëΩÔºåÊâÄ‰ª•ÊàëÂú∞ÂÅáË®≠ÊäìÂà∞Êó¢ÈÉΩ‰øÇÂ§©ÈëΩ
    
    # Regex: ÂØ¶Áî®\s*(\d+)Âëé.{0,200}?\$\s*([\d,]+)Ëê¨
    pattern = r'ÂØ¶Áî®\s*(\d+)\s*Âëé.{0,200}?\$\s*([\d,]+)\s*Ëê¨'
    
    matches = re.finditer(pattern, clean_html)
    
    for match in matches:
        try:
            size = int(match.group(1))
            price = int(match.group(2).replace(',', '')) * 10000
            
            # Âõ†ÁÇ∫‰∏≠ÂéüÂàóË°®Á∂ìÂ∏∏ÂîîÂØ´Â∫ßÊï∏ÔºåÊàëÂú∞Ë®≠ÁÇ∫ 0ÔºåÁ≠âÁî®Êà∂Ëá™Â∑± Click ÂÖ•ÂéªÁùá
            tower = 0 
            
            # ÂòóË©¶ÊâæÊ®ìÂ±§ (Âú®ÂåπÈÖçÊñáÂ≠óÈôÑËøë)
            raw_text = match.group(0)
            floor = "??"
            f_match = re.search(r'(‰Ωé|‰∏≠|È´ò)Â±§', raw_text) # ‰∏≠ÂéüÂèØËÉΩÂØ´Âú®ÂâçÈù¢ÔºåÈÄôË£°Áõ°ÈáèÊäì
            if f_match: floor = f_match.group(1)

            # ÊèèËø∞
            desc = f"Â§©ÈëΩ (‰∏≠ÂéüÁõ§) {size}Âëé"
            link = CONFIG["URL_CENTA"]

            listing = {
                "id": f"centa-{size}-{price}", # Áî® ÂëéÊï∏+ÂÉπÈå¢ ÂÅö ID
                "tower": tower, "floor": floor, "unit": "?",
                "price": price, "raw_desc": desc, "url": link,
                "source": "centanet", # Ê©ôËâ≤
                "sourceName": "‰∏≠Âéü",
                "scrapedAt": datetime.now().isoformat()
            }
            
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   ‚úÖ Centaline: {size}Âëé ${price/10000}Ëê¨")
        except: continue
        
    return listings

# --- Main ---
def main():
    log("üöÄ Starting Scraper v21 (3-Sources)...")
    
    seen_ids = set()
    try:
        if Path(CONFIG["CACHE_FILE"]).exists():
            data = json.loads(Path(CONFIG["CACHE_FILE"]).read_text())
            seen_ids = set(data.get("seen_ids", []))
    except: pass

    all_listings = []
    all_listings.extend(scrape_28hse())     # 28Hse
    all_listings.extend(scrape_squarefoot()) # Squarefoot
    all_listings.extend(scrape_centaline())  # Centaline
    
    log(f"üìä Total Found: {len(all_listings)}")
    
    # Sort: ÊúâÂ∫ßÊï∏ÊéíÂÖàÔºåÁÑ°Â∫ßÊï∏(0)ÊéíÂæå
    all_listings.sort(key=lambda x: (x['tower'] == 0, x['tower'], x['price']))

    # Email
    new_listings = [l for l in all_listings if l["id"] not in seen_ids]
    sender = os.environ.get("EMAIL_SENDER")
    password = os.environ.get("EMAIL_PASSWORD")
    
    if new_listings and sender and password:
        subject = f"üî• Â§©ÈëΩÊñ∞Áõ§ÈÄöÂ†± ({len(new_listings)})"
        lines = ["ÊúÄÊñ∞ÊîæÁõ§ (28Hse/Squarefoot/‰∏≠Âéü):", ""]
        for l in new_listings:
            t_str = f"Á¨¨ {l['tower']} Â∫ß" if l['tower'] > 0 else "Â§©ÈëΩ (Â∫ßÊï∏Êú™Ë©≥)"
            loc = f"{l['floor']}Â±§ {l['unit']}ÂÆ§" if l['unit'] != "?" else ""
            lines.append(f"üìç {t_str} {loc} | ${l['price']/10000:,.0f}Ëê¨ | {l['sourceName']}")
            lines.append(f"   {l['url']}")
            lines.append("")
        lines.append("Dashboard: https://spokenelam.github.io/sky-diamond-tracker/")
        
        msg = MIMEMultipart()
        msg['From'] = sender
        msg['To'] = ", ".join(CONFIG["EMAIL_RECIPIENTS"])
        msg['Subject'] = subject
        msg.attach(MIMEText("\n".join(lines), 'plain', 'utf-8'))
        
        try:
            with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
                server.login(sender, password)
                server.sendmail(sender, CONFIG["EMAIL_RECIPIENTS"], msg.as_string())
            log("üìß Email sent.")
        except: pass

    # Save
    current_ids = list(seen_ids)
    for l in all_listings:
        if l["id"] not in current_ids:
            current_ids.append(l["id"])
            
    Path("data").mkdir(exist_ok=True)
    output_data = {"lastUpdate": datetime.now().isoformat(), "listings": all_listings}
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    
    cache_data = {"last_run": datetime.now().isoformat(), "seen_ids": current_ids}
    Path(CONFIG["CACHE_FILE"]).write_text(json.dumps(cache_data, indent=2))
    log("üíæ Data saved.")

if __name__ == "__main__":
    main()
