#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v6 (All-in-One: 28Hse Pages + Ricacorp + Centaline + PropertyHK)
"""

import json
import urllib.request
import re
import random
import time
from datetime import datetime
from pathlib import Path

# --- Configuration ---
CONFIG = {
    # ç›®æ¨™åº§æ•¸ (Target Towers)
    "TARGET_TOWERS": [8, 9, 10, 11, 12, 13, 15, 16, 18],
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
}

def log(msg):
    print(msg, flush=True)

def get_headers():
    # éš¨æ©Ÿæ› User-Agent æ‰®çœŸäºº
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Edge/121.0.0.0',
    ]
    return {
        'User-Agent': random.choice(agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-HK,zh;q=0.9,en;q=0.8',
        'Cookie': 'locale=zh-hk', # å¼·åˆ¶ä¸­æ–‡
        'Referer': 'https://www.google.com/'
    }

def fetch_url(url):
    log(f"ğŸŒ Fetching: {url}")
    try:
        # éš¨æ©Ÿå»¶é² 1-3 ç§’ï¼Œæ¸›ä½è¢«å°æ©Ÿæœƒ
        time.sleep(random.uniform(1, 3))
        
        req = urllib.request.Request(url, headers=get_headers())
        with urllib.request.urlopen(req, timeout=20) as resp:
            return resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        # å¦‚æœæ˜¯ 403 Forbiddenï¼Œé€šå¸¸æ˜¯è¢«é˜²æ¯’ç‰†æ“‹äº†
        error_msg = str(e)
        if "403" in error_msg:
            log(f"âš ï¸ Access Denied (403) by {url} - IP Blocked")
        else:
            log(f"âŒ Error fetching {url}: {e}")
        return None

# =============================================================================
# 1. 28Hse (æ”¯æ´åˆ†é )
# =============================================================================
def scrape_28hse():
    log("--- Scraping 28Hse (Pages 1-3) ---")
    base_url = "https://www.28hse.com/utf8/buy/residential/a3/dg45/c22902"
    listings = []
    
    # æŠ“å– 1 åˆ° 3 é 
    for page in range(1, 4):
        # çµ„åˆ URL: ç¬¬ä¸€é ä¸ç”¨åŠ  page
        url = base_url if page == 1 else f"{base_url}/page-{page}"
        log(f"   > Processing Page {page}...")
        
        html = fetch_url(url)
        if not html: continue

        # æª¢æŸ¥æ˜¯å¦æœ€å¾Œä¸€é  (å¦‚æœé é¢æ²’æœ‰æ”¾ç›¤ item)
        if 'class="item' not in html:
            log("   > No more items, stopping.")
            break

        chunks = re.split(r'class="item', html)
        found_on_page = 0
        
        for chunk in chunks[1:]:
            try:
                # æ‰¾åº§æ•¸
                tower_match = re.search(r'(?:ç¬¬|Block|T)\s*(\d+)\s*åº§?', chunk, re.IGNORECASE)
                # æ‰¾åƒ¹éŒ¢
                price_match = re.search(r'\$\s*([\d,]+(?:\.\d+)?)\s*è¬', chunk)
                if not price_match: # å¾Œå‚™åƒ¹éŒ¢æ ¼å¼
                    price_match = re.search(r'å”®\s*([\d,]+(?:\.\d+)?)\s*è¬', chunk)

                if tower_match and price_match:
                    tower = int(tower_match.group(1))
                    price_str = price_match.group(1).replace(',', '')
                    price = int(float(price_str) * 10000)

                    if tower in CONFIG["TARGET_TOWERS"]:
                        # æ‰¾æè¿°
                        clean_text = re.sub(r'<[^>]+>', ' ', chunk)
                        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
                        desc = clean_text[:60] + "..."
                        
                        # æ‰¾è©³ç´°é€£çµ
                        link_match = re.search(r'href="([^"]+)"', chunk)
                        link = link_match.group(1) if link_match else url

                        listing = {
                            "id": f"28hse-{tower}-{price}-{page}",
                            "tower": tower,
                            "floor": "??", "unit": "?", "size": 0, "rooms": 0,
                            "price": price, "pricePerFt": 0,
                            "raw_desc": desc,
                            "url": link,
                            "source": "hse28",
                            "sourceName": "28Hse",
                            "scrapedAt": datetime.now().isoformat()
                        }
                        
                        if not any(l["id"] == listing["id"] for l in listings):
                            listings.append(listing)
                            found_on_page += 1
            except:
                continue
        log(f"   > Found {found_on_page} items on page {page}")

    return listings

# =============================================================================
# 2. åˆ©å˜‰é–£ Ricacorp
# =============================================================================
def scrape_ricacorp():
    log("--- Scraping Ricacorp ---")
    url = "https://www.ricacorp.com/zh-hk/property/list/buy/%E5%A4%A9%E9%91%BD-estate-%E5%A4%A7%E5%9F%94%E5%8D%8A%E5%B1%B1-hma-hk"
    html = fetch_url(url)
    if not html: return []
    
    listings = []
    # åˆ‡å‰²å€å¡Š
    chunks = re.split(r'class="property-card', html)
    
    for chunk in chunks[1:]:
        try:
            # åˆ©å˜‰é–£æ ¼å¼: "ç¬¬8åº§"
            tower_match = re.search(r'ç¬¬\s*(\d+)\s*åº§', chunk)
            
            # åƒ¹éŒ¢: $ 638 è¬
            price_match = re.search(r'\$\s*([\d,]+)\s*è¬', chunk)
            
            # å‘æ•¸
            size_match = re.search(r'(\d+)\s*å‘', chunk)
            
            if tower_match and price_match:
                tower = int(tower_match.group(1))
                price_str = price_match.group(1).replace(',', '')
                price = int(price_str) * 10000
                size = int(size_match.group(1)) if size_match else 0
                
                if tower in CONFIG["TARGET_TOWERS"]:
                    # æ‰¾é€£çµ
                    link_match = re.search(r'href="([^"]+)"', chunk)
                    link = "https://www.ricacorp.com" + link_match.group(1) if link_match else url
                    
                    listing = {
                        "id": f"rica-{tower}-{price}",
                        "tower": tower,
                        "floor": "??", "unit": "?",
                        "size": size,
                        "rooms": 0,
                        "price": price,
                        "pricePerFt": price // size if size > 0 else 0,
                        "raw_desc": f"ç¬¬{tower}åº§ (åˆ©å˜‰é–£)",
                        "url": link,
                        "source": "centanet", # å€Ÿç”¨æ©™è‰² style (æˆ–æ”¹ç”¨ hkp ç´«è‰²)
                        "sourceName": "Ricacorp",
                        "scrapedAt": datetime.now().isoformat()
                    }
                    if not any(l["id"] == listing["id"] for l in listings):
                        listings.append(listing)
                        log(f"   âœ… Ricacorp Found: T{tower} ${price_str}è¬")
        except:
            continue
            
    return listings

# =============================================================================
# 3. ä¸­åŸ Centaline (æ¥µé›£æŠ“ï¼ŒBest Effort)
# =============================================================================
def scrape_centaline():
    log("--- Scraping Centaline ---")
    url = "https://hk.centanet.com/findproperty/list/buy/%E5%A4%A9%E9%91%BD_2-DEPPWPPJPB"
    html = fetch_url(url)
    if not html: return []
    
    listings = []
    # ä¸­åŸæ˜¯ React Appï¼ŒHTML è£¡é¢é€šå¸¸åªæœ‰ä¸€å † JSON data åœ¨ <script> æ¨™ç±¤è£¡
    # æˆ‘å€‘å˜—è©¦åœ¨æºä»£ç¢¼è£¡æ‰¾ "ç¬¬Xåº§" çš„è¹¤è·¡
    
    # ç°¡å–® Regex æƒæå…¨æ–‡
    # æ ¼å¼å¯èƒ½ä¿‚: "title":"ç¬¬8åº§..." æˆ–è€…ç´”æ–‡å­—
    matches = re.findall(r'ç¬¬\s*(\d+)\s*åº§.{0,100}?\$([\d,]+)è¬', html)
    
    for match in matches:
        try:
            tower = int(match[0])
            price_str = match[1].replace(',', '')
            price = int(price_str) * 10000
            
            if tower in CONFIG["TARGET_TOWERS"]:
                listing = {
                    "id": f"centa-{tower}-{price}",
                    "tower": tower,
                    "floor": "??", "unit": "?", "size": 0, "rooms": 0,
                    "price": price, "pricePerFt": 0,
                    "raw_desc": f"ç¬¬{tower}åº§ (ä¸­åŸ)",
                    "url": url,
                    "source": "centanet",
                    "sourceName": "Centaline",
                    "scrapedAt": datetime.now().isoformat()
                }
                if not any(l["id"] == listing["id"] for l in listings):
                    listings.append(listing)
                    log(f"   âœ… Centaline Found: T{tower} ${price_str}è¬")
        except:
            continue
            
    return listings

# =============================================================================
# 4. Property.hk (ä¿åº•)
# =============================================================================
def scrape_property_hk():
    log("--- Scraping Property.hk ---")
    url = "https://www.property.hk/buy/search/%E5%A4%A9%E9%91%BD/"
    html = fetch_url(url)
    if not html: return []
    listings = []
    rows = html.split('</tr>')
    for row in rows:
        try:
            tower_match = re.search(r'ç¬¬\s*(\d+)\s*åº§', row)
            price_match = re.search(r'(\d{3,5})\s*è¬', row)
            size_match = re.search(r'(\d{3,4})\s*å‘', row)
            if tower_match and price_match:
                tower = int(tower_match.group(1))
                price = int(price_match.group(1)) * 10000
                size = int(size_match.group(1)) if size_match else 0
                if tower in CONFIG["TARGET_TOWERS"]:
                    link_match = re.search(r'href="([^"]+)"', row)
                    link = "https://www.property.hk" + link_match.group(1) if link_match else url
                    listing = {
                        "id": f"phk-{tower}-{price}",
                        "tower": tower, "floor": "??", "unit": "?", "size": size, "rooms": 0,
                        "price": price, "pricePerFt": price // size if size > 0 else 0,
                        "raw_desc": f"ç¬¬{tower}åº§ (Property.hk)",
                        "url": link, "source": "hkp", "sourceName": "Property.hk",
                        "scrapedAt": datetime.now().isoformat()
                    }
                    if not any(l["id"] == listing["id"] for l in listings):
                        listings.append(listing)
                        log(f"   âœ… Property.hk Found: T{tower} ${listing['price']//10000}è¬")
        except: continue
    return listings

# =============================================================================
# MAIN
# =============================================================================
def main():
    log("ğŸš€ Starting Scraper v6 (All-in-One)...")
    all_listings = []
    
    # åŸ·è¡Œæ‰€æœ‰ Scraper
    all_listings.extend(scrape_28hse())     # 28Hse (å¤šé )
    all_listings.extend(scrape_ricacorp())  # åˆ©å˜‰é–£
    all_listings.extend(scrape_centaline()) # ä¸­åŸ (å¯èƒ½403)
    all_listings.extend(scrape_property_hk()) # Property.hk (ä¿åº•)
    
    log(f"ğŸ“Š Total Combined Listings: {len(all_listings)}")

    # æ’åºï¼šåƒ¹éŒ¢ä½åˆ°é«˜
    all_listings.sort(key=lambda x: x['price'])

    # Save
    Path("data").mkdir(exist_ok=True)
    output_data = {
        "lastUpdate": datetime.now().isoformat(),
        "listings": all_listings
    }
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    log("ğŸ’¾ Data saved.")

if __name__ == "__main__":
    main()
