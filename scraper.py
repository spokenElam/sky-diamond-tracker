#!/usr/bin/env python3
"""
å¤©é‘½ The Regent - Scraper v3 (Robust & Debug Mode)
"""

import json
import os
import sys
import traceback
import urllib.request
import re
import random
import time
from datetime import datetime
from pathlib import Path

# --- Configuration ---
CONFIG = {
    "TARGET_TOWERS": [8, 9, 10, 11, 12, 13, 15, 16, 18],
    "CACHE_FILE": "data/listings_cache.json",
    "OUTPUT_FILE": "data/listings.json",
    # é€™æ˜¯ä½ çš„ç›®æ¨™ URL (å¤©é‘½)
    "URL": "https://www.28hse.com/buy/residential/property/16716" 
}

# --- Helpers ---
def log(msg):
    print(msg, flush=True)

def get_random_user_agent():
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0'
    ]
    return random.choice(agents)

# --- Scraper Logic ---
def fetch_html(url):
    log(f"ğŸŒ Fetching URL: {url}")
    
    headers = {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-HK,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
        'Referer': 'https://www.28hse.com/',
        'Cookie': 'locale=zh-hk' # å˜—è©¦å¼·åˆ¶ä¸­æ–‡
    }

    try:
        req = urllib.request.Request(url, headers=headers)
        # éš¨æ©Ÿå»¶é²ï¼Œçœ‹èµ·ä¾†åƒçœŸäºº
        time.sleep(2) 
        with urllib.request.urlopen(req, timeout=30) as resp:
            html = resp.read().decode('utf-8', errors='ignore')
            return html
    except Exception as e:
        log(f"âŒ Network Error: {e}")
        return None

def parse_listings(html):
    listings = []
    
    # 1. æª¢æŸ¥æ˜¯å¦è¢«æ“‹ (Debug)
    title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    page_title = title_match.group(1) if title_match else "No Title Found"
    log(f"ğŸ“„ Page Title found: [{page_title}]")
    
    if "Security" in page_title or "Just a moment" in page_title or "Cloudflare" in page_title:
        log("âš ï¸ WARNING: GitHub IP might be blocked by Cloudflare.")
        return []

    # 2. å˜—è©¦ç”¨ç°¡å–®æš´åŠ›çš„æ–¹å¼æŠ“å– (Pattern A: å°‹æ‰¾åŒ…å« 'åº§' å’Œ 'è¬' çš„å€å¡Š)
    # é€™ç¨®å¯«æ³•æœƒå¿½ç•¥ HTML çµæ§‹ï¼Œç›´æ¥åœ¨æ–‡å­—æµä¸­å°‹æ‰¾ "æ•¸å­—+åº§ ... æ•¸å­—+è¬"
    # ä¾‹å¦‚: "8åº§ ... $800è¬"
    
    log("ğŸ” Trying extraction pattern...")
    
    # ç§»é™¤ HTML æ¨™ç±¤ï¼Œè½‰æˆç´”æ–‡å­—ä¾†åˆ†æï¼Œæ¸›å°‘çµæ§‹å¹²æ“¾
    clean_text = re.sub(r'<[^>]+>', ' ', html)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    
    # Pattern: æ‰¾å°‹ "ç¬¬Xåº§" æˆ– "Xåº§"ï¼Œå¾Œé¢è·Ÿè‘—åƒ¹éŒ¢
    # å®¹è¨±ä¸­é–“é–“éš” 100 å€‹å­—å…ƒ
    pattern = r'(\d+)\s*åº§.{0,100}?\$?([\d,]+(?:\.\d+)?)\s*è¬'
    
    matches = re.findall(pattern, clean_text)
    log(f"   Found {len(matches)} raw matches")

    for match in matches:
        try:
            tower_str = match[0]
            price_str = match[1].replace(',', '')
            
            tower = int(tower_str)
            price = int(float(price_str) * 10000)
            
            # éæ¿¾åº§æ•¸
            if tower not in CONFIG["TARGET_TOWERS"]:
                continue
                
            # å»ºç«‹ ID
            listing_id = f"{tower}-{price}"
            
            listing = {
                "id": listing_id,
                "tower": tower,
                "floor": "??", # å¯¬é¬†æ¨¡å¼ä¸å¼·æ±‚æ¨“å±¤
                "unit": "?",
                "size": 0,
                "rooms": 0,
                "price": price,
                "pricePerFt": 0,
                "raw_desc": f"ç¬¬{tower}åº§ (HK${price_str}è¬)",
                "url": CONFIG["URL"],
                "source": "28hse",
                "sourceName": "28Hse",
                "scrapedAt": datetime.now().isoformat()
            }
            
            # å»é‡
            if not any(l["id"] == listing["id"] for l in listings):
                listings.append(listing)
                log(f"   âœ… Matched: Tower {tower} @ ${price_str}è¬")

        except Exception as e:
            continue

    return listings

# --- Main Execution ---
def main():
    log("ğŸš€ Starting Scraper v3...")
    
    # Load Cache
    seen_ids = set()
    try:
        if Path(CONFIG["CACHE_FILE"]).exists():
            data = json.loads(Path(CONFIG["CACHE_FILE"]).read_text())
            seen_ids = set(data.get("seen_ids", []))
    except:
        pass

    # Fetch & Parse
    html = fetch_html(CONFIG["URL"])
    current_listings = []
    
    if html:
        current_listings = parse_listings(html)
    else:
        log("âŒ No HTML content retrieved.")

    log(f"ğŸ“Š Total Listings Found: {len(current_listings)}")

    # Update Data Files
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    Path("data").mkdir(exist_ok=True)
    
    # 1. Update JSON for Website (ç¸½æ˜¯è¦†è“‹ï¼Œç¢ºä¿ç¶²ç«™é¡¯ç¤ºæœ€æ–°)
    output_data = {
        "lastUpdate": datetime.now().isoformat(),
        "listings": current_listings
    }
    Path(CONFIG["OUTPUT_FILE"]).write_text(json.dumps(output_data, ensure_ascii=False, indent=2))
    
    # 2. Update Cache (ä¿ç•™æ­·å²ç´€éŒ„)
    new_ids = [l["id"] for l in current_listings]
    seen_ids.update(new_ids)
    
    cache_data = {
        "last_run": datetime.now().isoformat(),
        "seen_ids": list(seen_ids)
    }
    Path(CONFIG["CACHE_FILE"]).write_text(json.dumps(cache_data, indent=2))
    
    log("ğŸ’¾ Data saved successfully.")

    # Email Logic (Optional: åªæœ‰åœ¨çœŸçš„æœ‰æ–°ç›¤æ™‚æ‰åœ¨é€™è£¡åŠ )
    # ... (ä¿æŒä½ çš„ YAML è™•ç† email æˆ–åœ¨æ­¤è™•åŠ å…¥ï¼Œç›®å‰å…ˆå°ˆæ³¨æ–¼ä¿®å¾©æŠ“å–)

if __name__ == "__main__":
    main()
